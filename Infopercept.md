### ğŸ¢ **Infopercept Consulting Pvt. Ltd. **
ğŸ› ï¸ **Role:** Backend AI Developer Interview
ğŸ“ **Location:** Ahmadabad, India (Remote)
ğŸ’¡ **Skills Required**: Prompt Engineering, Artificial Intelligence, Agentic AI, Backend, Programming 
ğŸ‘¤ **Recruiter name:**  Vidhi Raval

## What This Interview *Is*
- A **problem-solving evaluation** using a technical challenge
- A test of your ability to work in an **unfamiliar environment**
- A chance to see how well you **utilize LLMs as partners**, not just autocomplete tools
- An opportunity to demonstrate **clarity of thought**, adaptability, and ownership

---

## What This Interview *Is Not*
- Itâ€™s not a test of Rust, Python, or any specific language knowledge (unless stated)
- Itâ€™s not about memorization, trick questions, or syntax trivia
- Itâ€™s not about getting everything rightâ€”itâ€™s about **how you handle what goes wrong**

---

## Tools You Can Use
- You may use **ChatGPT, GitHub Copilot, or any LLM of your choice**
- You may use **Docker, online IDEs (e.g., Replit, Gitpod, Codespaces)** to bypass local setup issues
- You may search for documentation, examples, or tutorialsâ€”**but you must understand what you're applying**

---

## What We'll Be Looking For
- **Prompting strategy** â€“ Are you asking the LLM smart questions? Are you iterating?
- **Problem-solving mindset** â€“ Can you break down a problem and build incrementally?
- **Ownership** â€“ Do you know when to adapt, ask, or pivot?
- **Communication** â€“ Can you explain what youâ€™re doing and why?
- **Recovery** â€“ How do you handle friction, errors, or failure?

---

## Recommendations Before the Interview
- Make sure your **development environment is ready** (or be ready to use Docker or a cloud IDE)
- Ensure you have **access to your preferred LLM assistant** (e.g., ChatGPT Plus, local model, etc.)
- Be prepared to **ask clarifying questions** or challenge assumptions if needed
- If you're unfamiliar with a language or stack, focus on **thinking clearly and prompting effectively**

---

## During the Interview
- You are encouraged to **talk through your thought process**
- If you hit a wall, say so. **Donâ€™t spinâ€”reflect.**
- If you make a design choice, explain your reasoning
- If you're using an LLM, share a few of your prompts. This helps us understand your approach


- **Round 1** â€“ Machine Coding only 1round (Virtual Video Call) 2 hr 
ğŸ‘¤ **Interviewer:** Nehal Dattani

Here is the task
## 0Â Â·Â Premise

A global logistics operator runs **5000** longâ€‘haul trucks.  
Every vehicle emits a compact **telemetry frame every five minutes** (Â±â€¯jitter) while enâ€‘route:

```jsonc
{
  "truck_id": "T0001",
  "trip_id": "TR0001",
  "ts_utc": "2025â€‘05â€‘30T04:15:00Z",
  "lat": 37.7749,
  "lon": -122.4194,
  "speed_kph": 76.4,
  "heading_deg": 90.0,
  "fuel_pct": 64,
  "cargo_temp_c": 5.2
}
```

Your assignment is to **simulate the fleet, ingest the fireâ€‘hose, enrich each frame with weather, and predict ETA** â€” all on a developer laptop.  
Lean on ChatGPT / Claude / etc. as much as you wish _during_ the session, but the resulting repo must:  

* compile and run offline;  
* keep infra strictly local (Docker, k3d, SQLite/PostgreSQL inside containers).

---

## 1Â Â·Â Repo Layout

```
supply_chain_eta_oracle/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ .env.example
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Makefile               # tiny helper targets
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs            # dispatch â€”Â do not edit
â”‚   â””â”€â”€ bin/
â”‚       â”œâ”€â”€ simulator.rs   # TODO  â† sectionÂ 2â€‘A
â”‚       â””â”€â”€ server.rs      # TODO  â† sectionÂ 2â€‘A/B/C
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ trucks.csv         # 5â€¯000 numbered plates (sample =Â 50)
â”‚   â”œâ”€â”€ trips.csv          # scheduled trips, ISOâ€‘8601 start, planned duration
â”‚   â””â”€â”€ weather_stub.json  # hourly WX snapshots for three corridors
â””â”€â”€ docs/
    â””â”€â”€ architecture.mmd   # PlantUML / Mermaid diagram placeholder
```

---

## 2Â Â·Â Task Matrix

The exercise is intentionally **layered**.  
Finish rows **A** and **B** first â€” go ahead _only_ after they work endâ€‘toâ€‘end.

| ID | Category | Acceptance criteria |
|----|-----------|--------------------|
| **Aâ€‘1** | **Basic â€”Â Simulator** | Binary `simulator` opens _N_ WebSockets (â€¯default **5000**) to `ENDPOINT`.<br/>*CLI flags* **--endpoint**, **--clients**, **--replayâ€‘speed** (1â€¯=â€¯real time), **--jitter-ms**.<br/>Replays `data/trips.csv`, computing lat/lon per tick with greatâ€‘circle interpolation.<br/>Reconnects with exponential backâ€‘off (2â€¯â†’â€¯30â€¯s).<br/>Throughput â‰¥â€¯1â€¯000â€¯msg/s in `--replayâ€‘speed 600` mode on a 4â€‘core Intel laptop. |
| **Aâ€‘2** | **Basic â€”Â Ingestion microservice** | Binary `server` exposes **/ingest** (WebSocket).<br/>Validates & ACKs each frame.<br/>Persists raw JSON into **SQLite** (`telemetry_raw` table).<br/>Graceful shutdown (ctrlâ€‘C) flushes outstanding writes. |
| **Bâ€‘1** | **Prod â€”Â Containerisation & scaling** | Provide `Dockerfile` for **simulator** _and_ **server**; both build inâ€¯<â€¯5â€¯min on M1.<br/>`docker-compose up --scale simulator=5` spins up 5â€¯Ã—â€¯1â€¯000â€‘client sims plus one server and local **PostgreSQL 16**.<br/>Env vars `CLIENTS_PER_SIM`, `SERVER_WORKERS` control scale. |
| **Bâ€‘2** | **Prod â€”Â Durable persistence** | Switch to PostgreSQL; writeâ€¯SQLâ€¯x migrations.<br/>Schema: `trucks`, `trips`, `telemetry`, `weather`, `eta_predictions`.<br/>Primary keys & sensible indices (ts, truck). |
| **Bâ€‘3** | **Prod â€”Â Weather enrichment** | Implement `WeatherProvider` trait.<br/>Default impl hits **Openâ€‘Meteo** _(live)_ **or** reads `weather_stub.json` when `OFFLINE_MODE=1`.<br/>Enriched columns: `wx_temp_c`, `wx_wind_mps`, `wx_precip_mm`. |
| **Câ€‘1** | **AdvancedÂ (chooseÂ 3)** â€”Â ML ETA model | Offline notebook / script trains gradientâ€‘boost model (_xgboostâ€‘rs_ / Pythonâ€‘onnx).<br/>Load ONNX in `server`; endpoint `/eta/:trip_id` returns ETA + Î” vs schedule.<br/>ScoreÂ â‰¤â€¯5â€¯% MAPE on `sample_eval.csv`. |
| **Câ€‘2** | Backâ€‘pressure & jitter control | Bounded channel; dropâ€‘oldest with metric `telemetry_dropped_total`; adaptive sleep to smooth spikes. |
| **Câ€‘3** | Observability | `/metrics` in Prometheus exposition format; include ingestion QPS, DB latency histogram.<br/>Bundled Grafana dashboard JSON. |
| **Câ€‘4** | Graceful degradation | Persist to **sled** queue when DB offline; background flusher on recovery. |
| **Câ€‘5** | Load test | Provide **k6** or Rustâ€‘based script hitting 30â€¯min, 5â€¯000 VRs; output HTML report. |
| **Câ€‘6** | Kubernetes | k3d cluster + Helm chart or Kustomize; HPA on CPU & custom `ingest_qps`. |
| **Câ€‘7** | Secure transport | Mutual TLS with selfâ€‘signed CA; bearer token auth on top; rotate secrets every start. |

> **Pick exactly three** items from C and declare them in **README\#Chosenâ€‘Advancedâ€‘Items** before coding.

---

## 3Â Â·Â What We Evaluate

| Axis | Weight |
|------|--------|
| Correctness & determinism | 35â€¯% |
| Throughput vs resource use | 20â€¯% |
| Code quality & idiomatic Rust | 20â€¯% |
| Architecture & tradeâ€‘off reasoning | 15â€¯% |
| Documentation (README, diagrams, tests) | 10â€¯% |

Hard limits:

* CPU â‰¤â€¯4 cores, RAM â‰¤â€¯16â€¯GB, run time of evaluation â‰¤â€¯15â€¯min.

---

## 4Â Â·Â QuickÂ start

```bash
# â¶Â bootstrap toolâ€‘chain
rustup default stable

# â·Â run stubs (they just print 'implement me')
cargo run --bin server
cargo run --bin simulator

# â¸Â spin containers (after you implement Bâ€‘1)
make compose-up      # shorthand for dockerâ€‘composeÂ up â€‘d

# â¹Â run load test (if you attempt Câ€‘5)
make loadtest
```

Detailed steps live in **HOW_TO_RUN.md**.

---

## 5Â Â·Â Deliverables

1. **All your** prompts and their output
2. **Git repo** (push to GitHubâ€¯/â€¯GitLab) + commit history.  
3. `supply_chain_eta_oracle.zip` with `target/release` binaries for airâ€‘gapped review.  
4. Oneâ€‘page architecture diagram (Mermaid / PlantUML) in `docs/architecture.mmd`.  
5. Short screencast (â‰¤â€¯3Â min, optional) demoing the stack at 10â€¯000â€¯msg/s.

---

## 6Â Â·Â Submission & Timeâ€‘box

*You have 2-3 consecutive hours once the timer starts.*  
Treat it like prodâ€‘outage triage: ship an MVP first, then harden / polish.

_Thatâ€™s it â€” happy hacking!_
